#!/usr/bin/env python
# -*- coding: utf-8 -*-
from evaltools.com_tools.frfw_tools import *
import numpy as np
import matplotlib.pyplot as plt
import os
import sys
import argparse
import json
import warnings
warnings.filterwarnings("ignore")


SCORE_SETTINGS = {
    # [score_max_value, score_zero_value, weight]
    'ce': [0, 2, 90, 1/6],
    'he': [0, 1, 90, 1/6],
    'eag': [0, 2, 90, 1/6],
    'rda_robot': [0, 0.5, 90, 1/6],
    'rpa_robot': [0, 0.5, 90, 1/6],
    'rda_exhibit': [0, 0.5, 90, 1/6],
    'rpa_exhibit': [0, 0.5, 90, 1/6]
}
EAG_THR = 1.0


def main(eval_middle_filenames, sections_filename):
    """
    Calculate overall evaluation results from intermediate accuracy evaluation files.

    This function aggregates evaluation data and produces summary metrics.

    Parameters
    ----------
    eval_middle_filenames: String
        FileName of the intermediate evaluation file generated by do_eval.py
    sections_filename: String
        FileName of the configuration file for calculating results within the specified time range

    Returns
    -------
    result_dict: dictionary
        Evaluation Result

    Outputs
    -------
    result.json: json
        Dictionary storing evaluation results by key metrics
    """
    df_m = pd.concat([load_csv(filename)
                     for filename in eval_middle_filenames], ignore_index=True)

    # Set timerange
    if sections_filename is None:
        time_intervals = [[df_m.index.min(), df_m.index.max()]]
    else:
        time_intervals = load_json(sections_filename)['sections']
        if len(time_intervals) < 1:
            time_intervals = [[df_m.index.min(), df_m.index.max()]]

    result_dict = {}
    type_list = np.unique(df_m.type)
    df_overall = pd.DataFrame()
    for interval in time_intervals:
        s, e = interval
        df_m_interval = df_m[(s <= df_m.index) & (df_m.index <= e)]

        df_overall = pd.concat([df_overall, df_m_interval])

    df_overall.sort_index(inplace=True)

    for type_tag in type_list:
        score_percentile = 90
        df_type = df_overall[df_overall.type == type_tag]

        if type_tag == 'oe' or type_tag == 'fe':
            res = get_evalresult_boolean(df_type)
        else:
            df_type = handle_postprocessing_for_each_evaluation(
                df_type, type_tag)
            if type_tag in SCORE_SETTINGS.keys():
                score_percentile = SCORE_SETTINGS[type_tag][2]
            res = get_evalresult_traj(df_type, score_percentile)

        result_dict[F'{type_tag}'] = res

    # For xDR_Challenge_2025
    # Change grouping rpa_12, rpa_21 --> rpa_exhibit, rpa_robot
    if 'rel_target' in df_overall.columns:
        for type_tag in ['rda', 'rpa']:
            mask = df_overall['type'].str.contains(type_tag, na=False)
            df_type = df_overall[mask]
            try:
                del result_dict[F'{type_tag}_12']
                del result_dict[F'{type_tag}_21']
            except:
                pass

            rel_targets = np.unique(df_type.rel_target)
            for rel_target in rel_targets:
                score_percentile = 90
                df_rel_target = df_type[df_type.rel_target == rel_target]

                if rel_target in SCORE_SETTINGS.keys():
                    score_percentile = SCORE_SETTINGS[rel_target][2]
                res = get_evalresult_traj(df_rel_target, score_percentile)

                result_dict[F'{type_tag}_{rel_target}'] = res

    return result_dict


def handle_postprocessing_for_each_evaluation(df, type_tag):
    if type_tag == 'eag':
        def decode_value_eag(row):
            s_list = row.split(';')
            s_list = [float(s) for s in s_list]
            return s_list

        values_series = df['value'].apply(decode_value_eag)
        values_arr = np.stack(values_series.to_numpy())

        df = df[values_arr[:, 1] >= EAG_THR]
        values_arr = values_arr[values_arr[:, 1] >= EAG_THR]

        t_eag_arr = values_arr[:, 0]/values_arr[:, 1]

        df['value'] = t_eag_arr

        return df

    elif type_tag == 'he':
        df['value'] = np.abs(df.value.astype(float))
        return df

    else:
        return df


def get_evalresult_traj(df_type, score_percentile=90):
    df_type['value'] = df_type['value'].astype(float)

    result = {}
    result['avg'] = df_type.value.mean()
    result['median'] = df_type.value.median()
    result['min'] = df_type.value.min()
    result['max'] = df_type.value.max()
    result['per50'] = np.percentile(df_type.value, 50)
    result['per75'] = np.percentile(df_type.value, 75)
    result['per90'] = np.percentile(df_type.value, 90)
    result['per95'] = np.percentile(df_type.value, 95)

    result['score'] = np.percentile(df_type.value, score_percentile)

    return result


def get_evalresult_boolean(df_type):
    df_type['value'] = df_type['value'].astype(float)
    # return len(df_type)/len(df_type[df_type.value == 1])
    return np.sum(df_type.value)/len(df_type)


def calc_Competition_Score(result_dict, score_setting=None, output_dir='./'):
    """
    Calculate overall evaluation score for the xDR_Challenge_2025 competition

    Parameters
    ----------
    result_dict : Dictionary
    score_setting : Dictionary
        Score settings [score_max_value, score_zero_value, percentile, weight]
    output_dir : String
        Output directory name

    Returns
    -------
    result_dict : Dictionary
        Dictionary storing evaluation results by key metrics
    """
    if score_setting is None:
        score_setting = SCORE_SETTINGS

    Score = np.sum([calc_score_per_evaluation(etype, score_setting, result_dict, True)[0]
                   for etype in score_setting.keys()])

    result_dict['Score'] = Score
    print('--------------------')
    print(F'COMPETITION SCORE : {Score}')

    plot_Score_bar(result_dict, score_setting, output_dir)

    return result_dict


def calc_score_per_evaluation(etype, score_setting, result_dict, verbose=False):
    score_100 = score_setting[etype][0]
    score_0 = score_setting[etype][1]

    score = 100/(score_100-score_0) * \
        result_dict[etype]['score'] + 100
    w_score = score_setting[etype][3] * score

    if verbose:
        print(F"【{etype}】")
        print(F'SCORE : {score} / W_SCORE : {w_score}')
    return w_score, score


def plot_Score_bar(result, score_setting, output_dir='./'):
    labels = []
    values = []
    values_weighted = []
    for key, value in result.items():
        if key in score_setting.keys():
            w_score_etype, score_etype = calc_score_per_evaluation(
                key, score_setting, result)
            labels.append(key)
            values.append(score_etype)
            values_weighted.append(w_score_etype)

    fig, ax = plt.subplots(1, 2, figsize=(12, 8))

    TotalScore = result['Score']
    Weights = [f"{x[2]:.3f}" for x in score_setting.values()]
    fig.suptitle(F'Total Score:{TotalScore}, weights={Weights}')

    ax[0].bar(labels, values)
    ax[0].set_ylabel('Score (unweighted)')
    ax[0].set_xticks(labels)
    ax[0].set_xticklabels(['CE95', 'HE95', 'EAG95', 'RDA_robot95',
                          'RPA_robot95', 'RDA_exhibit95', 'RPA_exhibit95'], fontsize=8)

    ax[1].bar(labels, values_weighted)
    ax[1].set_ylabel('Score (weighted)')
    ax[1].set_xticks(labels)
    ax[1].set_xticklabels(['CE95', 'HE95', 'EAG95', 'RDA_robot95',
                          'RPA_robot95', 'RDA_exhibit95', 'RPA_exhibit95'], fontsize=8)

    plt.tight_layout()
    os.makedirs(output_dir, exist_ok=True)
    plt.savefig(F'{output_dir}{os.sep}Score_graph.png', dpi=200)


def main_cl():
    parser = argparse.ArgumentParser(
        description="Calculate the overall evaluation result score")

    parser.add_argument('-eval_middle_files', '-m', nargs='+',
                        help="Filename of the intermediate evaluation result (absolute or relative)")
    parser.add_argument('--output_json', '-o', default="./",
                        help="Output json file name")
    parser.add_argument('-sections_file', '-s',
                        help="Filename of the time intervals used for score computation")
    args = parser.parse_args()

    if not args.eval_middle_files:
        print("Error: No input files provided.")
        sys.exit(1)

    result_dict = main(args.eval_middle_files, args.sections_file)

    result_dict = calc_Competition_Score(
        result_dict, output_dir=args.output_json)

    os.makedirs(args.output_json, exist_ok=True)
    with open(f'{args.output_json}eval_summary.json', 'w', encoding='utf-8') as jf:
        json.dump(result_dict, jf, ensure_ascii=False, indent=4)


if __name__ == '__main__':

    main_cl()
