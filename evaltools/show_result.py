#!/usr/bin/env python
# -*- coding: utf-8 -*-
from evaltools.com_tools.frfw_tools import *
import numpy as np
import matplotlib.pyplot as plt
import os
import sys
import argparse
import json
import warnings
warnings.filterwarnings("ignore")


SCORE_SETTINGS = {
    # 各キーに対し、辞書のリストで設定
    # 各辞書は:
    #   - "max": スコア100点となる評価値
    #   - "zero": スコア0点となる評価値
    #   - "weight": 総合点での重み
    #   - "stat": "percentile" | "mean" | "median"（どの統計量で評価値を取るか）
    #   - "q": stat=="percentile" のときのみ使用（0-100）
    'ce':  [ {'max': 0, 'zero': 10,     'weight': 1/7, 'stat': 'percentile', 'q': 95} ],
    'he':  [ {'max': 0, 'zero': np.pi/2,'weight': 1/7, 'stat': 'percentile', 'q': 50} ],
    'eag': [ {'max': 0, 'zero': 0.25,      'weight': 1/7, 'stat': 'percentile', 'q': 50} ],
    'rda_robot':   [ {'max': 0, 'zero': 1,      'weight': 1/7, 'stat': 'percentile', 'q': 50} ],
    'rpa_robot':   [ {'max': 0, 'zero': np.pi,'weight': 1/7, 'stat': 'percentile', 'q': 50} ],
    'rda_exhibit': [ {'max': 0, 'zero': 1,      'weight': 1/7, 'stat': 'percentile', 'q': 50} ],
    'rpa_exhibit': [ {'max': 0, 'zero': np.pi,'weight': 1/7, 'stat': 'percentile', 'q': 50} ],
}
EAG_THR = 1.0


def main(eval_middle_filenames, sections_filename, score_setting=None):
    """
    Calculate overall evaluation results from intermediate accuracy evaluation files.

    This function aggregates evaluation data and produces summary metrics.

    Parameters
    ----------
    eval_middle_filenames: String
        FileName of the intermediate evaluation file generated by do_eval.py
    sections_filename: String
        FileName of the configuration file for calculating results within the specified time range

    Returns
    -------
    result_dict: dictionary
        Evaluation Result

    Outputs
    -------
    result.json: json
        Dictionary storing evaluation results by key metrics
    """
    df_m = pd.concat([load_csv(filename)
                     for filename in eval_middle_filenames], ignore_index=True)

    # Set timerange
    if sections_filename is None:
        time_intervals = [[df_m.index.min(), df_m.index.max()]]
    else:
        time_intervals = load_json(sections_filename)['sections']
        if len(time_intervals) < 1:
            time_intervals = [[df_m.index.min(), df_m.index.max()]]

    if score_setting is None:
        score_setting = SCORE_SETTINGS

    result_dict = {}
    type_list = np.unique(df_m.type)
    df_overall = pd.DataFrame()
    for interval in time_intervals:
        s, e = interval
        df_m_interval = df_m[(s <= df_m.index) & (df_m.index <= e)]

        df_overall = pd.concat([df_overall, df_m_interval])

    df_overall.sort_index(inplace=True)

    for type_tag in type_list:
        df_type = df_overall[df_overall.type == type_tag]

        if type_tag == 'oe' or type_tag == 'fe':
            res = get_evalresult_boolean(df_type)
        else:
            df_type = handle_postprocessing_for_each_evaluation(
                df_type, type_tag)
            res = get_evalresult_traj(df_type, type_tag, score_setting)

        result_dict[F'{type_tag}'] = res

    # For xDR_Challenge_2025
    # Change grouping rpa_12, rpa_21 --> rpa_exhibit, rpa_robot
    if 'rel_target' in df_overall.columns:
        for type_tag in ['rda', 'rpa']:
            mask = df_overall['type'].str.contains(type_tag, na=False)
            df_type = df_overall[mask]
            try:
                del result_dict[F'{type_tag}_12']
                del result_dict[F'{type_tag}_21']
            except:
                pass

            rel_targets = np.unique(df_type.rel_target)
            for rel_target in rel_targets:
                df_rel_target = df_type[df_type.rel_target == rel_target]

                res = get_evalresult_traj(
                    df_rel_target, F'{type_tag}_{rel_target}', score_setting)

                result_dict[F'{type_tag}_{rel_target}'] = res

    return result_dict


def handle_postprocessing_for_each_evaluation(df, type_tag):
    """
    Calculate evaluation-specific data transformation

    Parameters
    ----------
    df : pandas.DataFrame
        Time series error for each evaluation
    type_tag : string
        evaluation type tag

    Returns
    -------
    df : pandas.DataFrame
        Time series error for each evaluation
    """
    if type_tag == 'eag':
        # value : [error_distance_from_gt];[ALIP_elapsed_time];[ALIP_elapsed_distance];[ALIP_elapsed_angle]
        def decode_value_eag(row):
            s_list = row.split(';')
            s_list = [float(s) for s in s_list]
            return s_list

        values_series = df['value'].apply(decode_value_eag)
        values_arr = np.stack(values_series.to_numpy())

        df = df[values_arr[:, 1] >= EAG_THR]
        values_arr = values_arr[values_arr[:, 1] >= EAG_THR]

        t_eag_arr = values_arr[:, 0]/values_arr[:, 1]

        df['value'] = t_eag_arr

        return df

    elif type_tag == 'he':
        df['value'] = np.abs(df.value.astype(float))
        return df

    else:
        return df


def _legacy_to_dict_list(entry):
    """
    旧形式 [max, zero, percentile, weight] を新形式の辞書リストに変換。
    percentileはstat='percentile', q=<値>として扱う。
    既に辞書リストならそのまま返す。
    """
    if isinstance(entry, list):
        # 旧形式 or 既に辞書リスト
        if len(entry) > 0 and isinstance(entry[0], dict):
            return entry  # 新形式
        # 旧形式 [max, zero, percentile, weight]
        if len(entry) == 4:
            max_v, zero_v, per, w = entry
            return [ {'max': max_v, 'zero': zero_v, 'weight': w, 'stat': 'percentile', 'q': per} ]
    elif isinstance(entry, dict):
        # 1個だけ辞書で来た場合も受ける
        return [entry]
    raise ValueError("Invalid SCORE_SETTINGS entry format.")


def _select_stat_value(df_series, stat='percentile', q=90):
    if stat == 'mean':
        return float(df_series.mean())
    elif stat == 'median':
        return float(df_series.median())
    elif stat == 'percentile':
        return float(np.percentile(df_series.to_numpy(), q))
    else:
        raise ValueError(f"Unsupported stat: {stat}")


def calc_score_value(eval_value, max_value, zero_value):
    """
    max_value(=100点), zero_value(=0点) を線形に結ぶスコア（上限下限はここではクリップしない）
    """
    return 100.0/(max_value - zero_value) * eval_value + 100.0


def get_evalresult_traj(df_type, etype, score_setting):
    df_type['value'] = df_type['value'].astype(float)

    result = {}
    result['avg'] = df_type.value.mean()
    result['median'] = df_type.value.median()
    result['min'] = df_type.value.min()
    result['max'] = df_type.value.max()
    result['per50'] = np.percentile(df_type.value, 50)
    result['per75'] = np.percentile(df_type.value, 75)
    result['per90'] = np.percentile(df_type.value, 90)
    result['per95'] = np.percentile(df_type.value, 95)

    if etype in score_setting.keys():
        cfg_list = _legacy_to_dict_list(score_setting[etype])

        per_cfg_scores = []
        for cfg in cfg_list:
            stat = cfg.get('stat', 'percentile')
            q = cfg.get('q', 90)
            max_v = cfg['max']
            zero_v = cfg['zero']

            eval_value = _select_stat_value(df_type.value, stat=stat, q=q)
            score_val = calc_score_value(eval_value, max_v, zero_v)
            per_cfg_scores.append(score_val)

        # 複数設定がある場合は平均
        result['score_detail'] = per_cfg_scores  # 参考用（配列）
        result['score'] = float(np.mean(per_cfg_scores))

    return result



def get_evalresult_boolean(df_type):
    df_type['value'] = df_type['value'].astype(float)
    # return len(df_type)/len(df_type[df_type.value == 1])
    return np.sum(df_type.value)/len(df_type)


def calc_Competition_Score(result_dict, score_setting=None, output_dir='./'):
    """
    Calculate overall evaluation score for the xDR_Challenge_2025 competition

    Parameters
    ----------
    result_dict : Dictionary
        Dictionary storing the results of each evaluation value
    score_setting : Dictionary
        Score settings [score_max_value, score_zero_value, percentile, weight]
    output_dir : String
        Output directory name

    Returns
    -------
    result_dict : Dictionary
        Dictionary storing evaluation results by key metrics
    """
    if score_setting is None:
        score_setting = SCORE_SETTINGS

    # etypeごとのweightを算出（新形式/旧形式対応）
    etype_weights = {}
    for etype, entry in score_setting.items():
        cfg_list = _legacy_to_dict_list(entry)
        weights = [cfg.get('weight', 0.0) for cfg in cfg_list]
        if len(weights) == 0:
            w = 0.0
        else:
            w = float(np.mean(weights))
        etype_weights[etype] = w

    # 総合スコア
    Score = 0.0
    for etype, res in result_dict.items():
        if etype in etype_weights and isinstance(res, dict) and 'score' in res:
            Score += res['score'] * etype_weights[etype]

    result_dict['Score'] = Score
    print('--------------------')
    # print(F'COMPETITION _SCORE : {_Score}')
    print(F'COMPETITION SCORE : {Score}')

    plot_Score_bar(result_dict, score_setting, output_dir)

    return result_dict


def calc_score_per_evaluation(etype, score_setting, eval_value, verbose=False):
    """
    Calculate the competition score (unweighted) for each evaluation function

    Parameters
    ----------
    etype : string
        evaluation type tag
    score_setting : dictionary
        Settings for score conversion
    eval_value : pandas.DataFrame
        Time series error for each evaluation
    verbose : boolean
        print score and weighted score 

    Returns
    -------
    score : float
        Competition score (unweighted)
    """
    score_100 = score_setting[etype][0]
    score_0 = score_setting[etype][1]

    score = 100/(score_100-score_0) * eval_value + 100

    if verbose:
        print(F"【{etype}】")
        print(F'SCORE : {score} / W_SCORE : {score * score_setting[etype][3]}')
    return score


def plot_Score_bar(result, score_setting, output_dir='./'):
    labels = []
    values = []
    values_weighted = []
    weights_for_title = []

    for key, value in result.items():
        if key in score_setting.keys() and isinstance(value, dict) and 'score' in value:
            cfg_list = _legacy_to_dict_list(score_setting[key])
            w = float(np.mean([c.get('weight', 0.0) for c in cfg_list])) if len(cfg_list) > 0 else 0.0

            labels.append(key)
            values.append(float(value['score']))
            values_weighted.append(float(value['score']) * w)
            weights_for_title.append(f"{w:.3f}")
    fig, ax = plt.subplots(1, 2, figsize=(12, 8))

    TotalScore = result.get('Score', 0.0)
    fig.suptitle(F'Total Score:{TotalScore:.3f}, weights={weights_for_title}')

    ax[0].bar(labels, values)
    ax[0].set_ylabel('Score (unweighted)')
    ax[0].set_xticks(labels)
    ax[0].set_xticklabels(labels, fontsize=8, rotation=20)
    ax[0].set_ylim(top=100)

    ax[1].bar(labels, values_weighted)
    ax[1].set_ylabel('Score (weighted)')
    ax[1].set_xticks(labels)
    ax[1].set_xticklabels(labels, fontsize=8, rotation=20)

    plt.tight_layout()
    os.makedirs(output_dir, exist_ok=True)
    plt.savefig(F'{output_dir}{os.sep}Score_graph.png', dpi=200)


def main_cl():
    parser = argparse.ArgumentParser(
        description="Calculate the overall evaluation result score")

    parser.add_argument('-eval_middle_files', '-m', nargs='+',
                        help="Filename of the intermediate evaluation result (absolute or relative)")
    parser.add_argument('--output_json', '-o', default="./",
                        help="Output json file name")
    parser.add_argument('-sections_file', '-s',
                        help="Filename of the time intervals used for score computation")
    args = parser.parse_args()

    if not args.eval_middle_files:
        print("Error: No input files provided.")
        sys.exit(1)

    result_dict = main(args.eval_middle_files, args.sections_file)

    result_dict = calc_Competition_Score(
        result_dict, output_dir=args.output_json)

    os.makedirs(args.output_json, exist_ok=True)
    with open(f'{args.output_json}eval_summary.json', 'w', encoding='utf-8') as jf:
        json.dump(result_dict, jf, ensure_ascii=False, indent=4)


if __name__ == '__main__':

    main_cl()
